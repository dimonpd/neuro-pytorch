import torch
import torch.nn as nn
import torch.utils.data as data
import torch.optim as optim

class LSTMToLinear(nn.Module):
    def forward(self, x):
        return x[1][0].squeeze(0)


model = nn.Sequential(
    nn.LSTM(1, 10, batch_first=True),
    LSTMToLinear(),
    nn.Linear(10, 1)
)

x = torch.linspace(-10, 10, 2000)
y = torch.cos(x) + 0.5 * torch.sin(5*x) + 0.1 * torch.randn_like(x) + 0.2 * x

total = len(x)      # общее количество отсчетов
train_size = 1000   # размер обучающей выборки
seq_length = 20     # число предыдущих отсчетов, по которым строится прогноз следующего значения

y.unsqueeze_(1)
train_data_y = torch.cat([y[i:i+seq_length] for i in range(train_size-seq_length)], dim=1)
train_targets = torch.tensor([y[i+seq_length].item() for i in range(train_size-seq_length)])

test_data_y = torch.cat([y[i:i+seq_length] for i in range(train_size-seq_length, total-seq_length)], dim=1)
test_targets = torch.tensor([y[i+seq_length].item() for i in range(train_size-seq_length, total-seq_length)])

d_train = data.TensorDataset(train_data_y.permute(1, 0), train_targets)
d_test = data.TensorDataset(test_data_y.permute(1, 0), test_targets)

train_data = data.DataLoader(d_train, batch_size=8, shuffle=True)
test_data = data.DataLoader(d_test, batch_size=len(d_test), shuffle=False)

optimizer = optim.RMSprop(params=model.parameters(), lr=0.01)
loss_func = nn.MSELoss()

epochs = 5 # число эпох
model.train()

for _e in range(epochs):
    for x_train, y_train in train_data:
        predict = model(x_train.unsqueeze(-1)).squeeze()
        loss = loss_func(predict, y_train)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

model.eval()
d, t = next(iter(test_data))
with torch.no_grad():
    predict = model(d.unsqueeze(-1)).squeeze()

Q = loss_func(predict, t).item()
