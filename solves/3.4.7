import torch
import torch.nn as nn
import torch.utils.data as data
import torch.optim as optim

# здесь продолжайте программу
model = nn.Sequential(
    nn.Conv2d(1, 32, 5, padding=2),
    nn.ReLU(inplace=True),
    nn.MaxPool2d(2),
    nn.Conv2d(32, 16, 3, padding=1),
    nn.ReLU(inplace=True),
    nn.MaxPool2d(2),
    nn.Flatten(),
    nn.Linear(1024, 1)
)

d_train, d_test = data.random_split(ds, [0.7, 0.3])
train_data = data.DataLoader(d_train, batch_size=16, shuffle=True)
test_data = data.DataLoader(d_test, batch_size=len(d_test), shuffle=False)

optimizer = optim.Adam(params=model.parameters(), lr=0.01, weight_decay=0.01)
loss_func = nn.BCEWithLogitsLoss()
epochs = 2
model.train()

for _e in range(epochs):
    for x_train, y_train in train_data:
        predict = model(x_train)
        loss = loss_func(predict, y_train.unsqueeze(-1))

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# тестирование обученной НС
model.eval()
x_test, y_test = next(iter(test_data))
with torch.no_grad():
    p = model(x_test)
    Q = torch.sum(p.sign().flatten() == (2 * y_test.flatten() - 1)).item()

Q = Q / len(d_test)
