import torch
import torch.nn as nn
import torch.utils.data as data
import torch.optim as optim


class CharsDataset(data.Dataset):
    def __init__(self, prev_chars=7):
        self.prev_chars = prev_chars

        self.lines = _global_var_text
        self.alphabet = set(("".join(self.lines)).lower())
        self.int_to_alpha = dict(enumerate(sorted(self.alphabet)))
        self.alpha_to_int = {b: a for a, b in self.int_to_alpha.items()}
        self.num_characters = len(self.alphabet)
        self.onehots = torch.eye(self.num_characters)

        data = []
        targets = []

        for i, t in enumerate(self.lines):
            t = t.lower()
            for item in range(len(t)-self.prev_chars):
                data.append([self.alpha_to_int[t[x]] for x in range(item, item + self.prev_chars)])
                targets.append(self.alpha_to_int[t[item+self.prev_chars]])

        self.data = torch.tensor(data)
        self.targets = torch.tensor(targets)

        self.length = len(data)

    def __getitem__(self, item):
        return self.onehots[self.data[item]], self.targets[item]

    def __len__(self):
        return self.length


class TextRNN(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.hidden_size = 32
        self.in_features = in_features
        self.out_features = out_features

        self.rnn = nn.RNN(in_features, self.hidden_size, batch_first=True)
        self.out = nn.Linear(self.hidden_size, out_features)

    def forward(self, x):
        x, h = self.rnn(x)
        y = self.out(h)
        return y


# сюда копируйте объекты d_train и train_data
d_train = CharsDataset(prev_chars=10)
train_data = data.DataLoader(d_train, batch_size=8, shuffle=True)

model = TextRNN(d_train.num_characters, d_train.num_characters)

optimizer = optim.Adam(params=model.parameters(), lr=0.01)
loss_func = nn.CrossEntropyLoss()

epochs = 1 # число эпох
model.train()

for _e in range(epochs):
    for x_train, y_train in train_data:
        predict = model(x_train).squeeze(0)
        loss = loss_func(predict, y_train.long())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

model.eval()
predict = "нейронная сеть ".lower() # начальная фраза
total = 20 # число прогнозируемых символов (дополнительно к начальной фразе)

for _ in range(total):
    _data = d_train.onehots[[d_train.alpha_to_int[predict[-x]] for x in range(d_train.prev_chars, 0, -1)]]
    with torch.no_grad():
        p = model(_data.unsqueeze(0)).squeeze(0)
    indx = torch.argmax(p, dim=1)
    predict += d_train.int_to_alpha[indx.item()]

print(predict)
